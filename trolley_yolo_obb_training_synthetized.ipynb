{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trolley YOLO-OBB Training (Synthetic Data)\n",
    "\n",
    "This notebook trains a YOLO-OBB model on synthetic data.\n",
    "\n",
    "**Workflow:**\n",
    "1. **Environment Setup**: Install and import required libraries.\n",
    "2. **Configuration**: Set constants and seeds.\n",
    "3. **Data Augmentation**: Define geometric and color augmentations.\n",
    "4. **Data Preparation**: Split synthetic data into Train/Val and apply augmentations.\n",
    "5. **Format Conversion**: Convert polygon annotations to OBB format.\n",
    "6. **Training**: Train the YOLOv8-OBB model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6246c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q ultralytics albumentations opencv-python numpy pyyaml tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0de4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a496991",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = [\".jpg\", \".jpeg\", \".png\"]\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acea43ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/albumentations/core/composition.py:331: UserWarning: Got processor for keypoints, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "kp_params = A.KeypointParams(format=\"xy\", remove_invisible=False)\n",
    "\n",
    "common_augs = {\n",
    "    \"brightness_contrast\": A.Compose([\n",
    "        A.RandomBrightnessContrast(p=0.3)\n",
    "    ], keypoint_params=kp_params),\n",
    "\n",
    "    \"hue_saturation\": A.Compose([\n",
    "        A.HueSaturationValue(p=0.3)\n",
    "    ], keypoint_params=kp_params),\n",
    "\n",
    "    \"gauss_noise\": A.Compose([\n",
    "        A.GaussNoise(p=0.2)\n",
    "    ], keypoint_params=kp_params),\n",
    "\n",
    "    \"motion_blur\": A.Compose([\n",
    "        A.MotionBlur(blur_limit=5, p=0.2)\n",
    "    ], keypoint_params=kp_params),\n",
    "}\n",
    "\n",
    "# Full Geometric Augmentations\n",
    "geo_full = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.Rotate(limit=(45, 45), border_mode=cv2.BORDER_CONSTANT, p=1.0),\n",
    "        A.Rotate(limit=(90, 90), border_mode=cv2.BORDER_CONSTANT, p=1.0),\n",
    "    ], p=0.7)\n",
    "], keypoint_params=kp_params)\n",
    "\n",
    "# Restricted Geometric Augmentations (Horizontal Flip only)\n",
    "geo_restricted = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5)\n",
    "], keypoint_params=kp_params)\n",
    "\n",
    "AUGMENTATIONS_FULL = {**common_augs, \"geometric\": geo_full}\n",
    "AUGMENTATIONS_RESTRICTED = {**common_augs, \"geometric\": geo_restricted}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "Functions for loading labels, converting polygon coordinates to Minimum Oriented Rectangles (OBB), and saving datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2074c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_file(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    return [list(map(float, l.strip().split())) for l in lines]\n",
    "\n",
    "\n",
    "def save_label_file(path, entries):\n",
    "    with open(path, \"w\") as f:\n",
    "        for cls, coords in entries:\n",
    "            f.write(\" \".join([str(cls)] + [f\"{c:.6f}\" for c in coords]) + \"\\n\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def polygon_to_minrect_norm(points, w, h):\n",
    "    if len(points) != 4:\n",
    "        return None\n",
    "\n",
    "    pts = np.array(points, dtype=np.float32)\n",
    "    rect = cv2.minAreaRect(pts)\n",
    "    box = cv2.boxPoints(rect)\n",
    "\n",
    "    norm = []\n",
    "    for x, y in box:\n",
    "        norm.append(x / w)\n",
    "        norm.append(y / h)\n",
    "\n",
    "    return norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing Pipeline\n",
    "Logic to split the dataset and apply augmentations to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00881dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_and_split(dataset_dirs, train_ratio=0.8):\n",
    "    all_train = []\n",
    "    all_val = []\n",
    "    \n",
    "    for ds in dataset_dirs:\n",
    "        images_dir = ds / \"images\"\n",
    "        labels_dir = ds / \"labels\"\n",
    "        source_id = ds.name\n",
    "        \n",
    "        # Validate pair existence\n",
    "        valid_pairs = []\n",
    "        image_files = [p for p in images_dir.iterdir() if p.suffix.lower() in IMG_EXTS]\n",
    "        \n",
    "        for img_p in image_files:\n",
    "            lab_p = labels_dir / f\"{img_p.stem}.txt\"\n",
    "            if lab_p.exists():\n",
    "                valid_pairs.append((img_p, lab_p, source_id))\n",
    "        \n",
    "        # Shuffle and Split\n",
    "        random.shuffle(valid_pairs)\n",
    "        split_idx = int(len(valid_pairs) * train_ratio)\n",
    "        \n",
    "        all_train.extend(valid_pairs[:split_idx])\n",
    "        all_val.extend(valid_pairs[split_idx:])\n",
    "        \n",
    "    return all_train, all_val\n",
    "\n",
    "def process_dataset(file_list, mode, output_root):\n",
    "    # output_root should be FINAL_DATASET\n",
    "    img_out_dir = output_root / \"images\" / mode\n",
    "    lab_out_dir = output_root / \"labels\" / mode\n",
    "    img_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    lab_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    excluded_folders = [\"Capture_as_our_dataset\", \"Capture_scene_8\"]\n",
    "    \n",
    "    for img_p, lab_p, source_id in tqdm(file_list, desc=f\"Processing {mode}\"):\n",
    "        # Determine Augmentations\n",
    "        if mode == \"train\":\n",
    "            if source_id in excluded_folders:\n",
    "                active_augs = AUGMENTATIONS_RESTRICTED\n",
    "            else:\n",
    "                active_augs = AUGMENTATIONS_FULL\n",
    "        else:\n",
    "            # Validation: No augmentation, just identity/format conversion\n",
    "            active_augs = {\"origin\": A.Compose([], keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False))}\n",
    "            \n",
    "        # Load Image and Label\n",
    "        img = cv2.imread(str(img_p))\n",
    "        if img is None: continue\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        labels = load_label_file(lab_p)\n",
    "        keypoints = []\n",
    "        for lbl in labels:\n",
    "            coords = lbl[1:]\n",
    "            for i in range(0, 8, 2):\n",
    "                keypoints.append((coords[i] * w, coords[i + 1] * h))\n",
    "                \n",
    "        # Apply Augmentations Loop\n",
    "        base_name = f\"{source_id}__{img_p.stem}\"\n",
    "        \n",
    "        for aug_name, aug in active_augs.items():\n",
    "            try:\n",
    "                out = aug(image=img, keypoints=keypoints)\n",
    "                if len(out[\"keypoints\"]) != len(keypoints): continue\n",
    "                \n",
    "                # Convert Back to OBB Norm\n",
    "                entries = []\n",
    "                for i, lbl in enumerate(labels):\n",
    "                    pts = out[\"keypoints\"][i*4 : (i+1)*4]\n",
    "                    rect = polygon_to_minrect_norm(pts, out[\"image\"].shape[1], out[\"image\"].shape[0])\n",
    "                    if rect:\n",
    "                        entries.append((int(lbl[0]), rect))\n",
    "                \n",
    "                if not entries: continue\n",
    "                \n",
    "                # Save\n",
    "                suffix = f\"_{aug_name}\" if mode == \"train\" else \"\"\n",
    "                new_name = f\"{base_name}{suffix}\"\n",
    "                \n",
    "                cv2.imwrite(str(img_out_dir / f\"{new_name}.jpg\"), out[\"image\"])\n",
    "                save_label_file(lab_out_dir / f\"{new_name}.txt\", entries)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_p.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c9e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 651 Train, 166 Val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 651/651 [00:06<00:00, 97.22it/s] \n",
      "Processing val: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 166/166 [00:00<00:00, 436.33it/s]\n"
     ]
    }
   ],
   "source": [
    "DATASETS = [\n",
    "    Path(\"Synthetized_Data/Capture_as_our_dataset\"),\n",
    "    Path(\"Synthetized_Data/Capture_scene_8\"),\n",
    "    Path(\"Synthetized_Data/Capture_zone_with_human_occ2\"),\n",
    "    Path(\"Synthetized_Data/scene_10\"),\n",
    "]\n",
    "\n",
    "FINAL_DATASET = Path(\"final_dataset_synthesized\")\n",
    "\n",
    "# 1. Split First\n",
    "train_files, val_files = gather_and_split(DATASETS, train_ratio=0.8)\n",
    "print(f\"Split: {len(train_files)} Train, {len(val_files)} Val\")\n",
    "\n",
    "# 2. Process Train (With Augmentation)\n",
    "process_dataset(train_files, mode=\"train\", output_root=FINAL_DATASET)\n",
    "\n",
    "# 3. Process Val (No Augmentation)\n",
    "process_dataset(val_files, mode=\"val\", output_root=FINAL_DATASET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset YAML Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9018af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml(out_dir, nc, names):\n",
    "    content = f\"\"\"\n",
    "path: {out_dir.resolve()}\n",
    "train: images/train\n",
    "val: images/val\n",
    "test: images/test\n",
    "\n",
    "nc: {nc}\n",
    "names: {names}\n",
    "\"\"\"\n",
    "    with open(out_dir / \"dataset_synthesized.yaml\", \"w\") as f:\n",
    "        f.write(content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d331673",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_yaml(FINAL_DATASET, nc=1, names=[\"object\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6478e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo26l-obb.pt\")\n",
    "\n",
    "model.train(\n",
    "    data=\"final_dataset_synthesized/dataset_synthesized.yaml\",\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=10,\n",
    "    workers=20,\n",
    "    optimizer=\"AdamW\",\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    momentum=0.937,\n",
    "    weight_decay=0.0005,\n",
    "    close_mosaic=15,\n",
    "    patience=25,\n",
    "    save_period=10,\n",
    "    val=True,\n",
    "    amp=True,\n",
    "    device=0,\n",
    "    hsv_h=0.015, hsv_s=0.7, hsv_v=0.4,\n",
    "    degrees=10.0,\n",
    "    translate=0.1,\n",
    "    scale=0.9,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    "    mixup=0.1,\n",
    "    erasing=0.3,\n",
    "    perspective=0.0005,\n",
    "    project=\"paper_experiment\",\n",
    "    name=\"YOLO26l_OBB_E1_Synthesized_only\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}